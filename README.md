<!-- #region -->
# Semantic fetal MR image synthesis using SPADE

This is a forked Github repository containing the code used for my semester project conducted in the Center for MR-Research, University Children's Hospital ZÃ¼rich and Swiss Federal Institute of Technology. The semester project was supervised by Prof. Dr. Andras Jakab and Prof. Dr. Ender Konukoglu.

In this semester project, [Med-DDPM](https://github.com/mobaidoctor/med-ddpm), a denoising diffusion probabilistic model, is employed to generate high-quality 3D MR images of the developing human brain, attempting to address some of the limitations of traditional GAN-based methods. The work focuses on developing and refining the existing Med-DDPM for semantic 3D MRI synthesis using brain tissue label maps and compares its performance with the [SPADE GAN](https://github.com/NVlabs/SPADE) method. Results demonstrate Med-DDPM's capabilities to generate realistic, anatomically detailed, coherent fetal MR images with improved statistical accuracy and signal clarity. However, it falls short in reliably and consistently producing these high-quality images compared to SPADE GAN. This exploration contributes to the field of medical imaging by offering a novel approach for generating realistic fetal MRIs from a label map representing anatomical brain tissues.

My forked Github reposity of Med-DDPM: [feta-ddpm](https://github.com/dagbelese/feta-ddpm)

<!-- #endregion -->

## Installation

Clone this repo.
```bash
git clone https://github.com/dagbelese/feta-SPADE.git
cd SPADE/
```

This code requires PyTorch 1.0 and python 3+. Please install dependencies by
```bash
pip install -r requirements.txt
```

This code also requires the Synchronized-BatchNorm-PyTorch rep.
```
cd models/networks/
git clone https://github.com/vacancy/Synchronized-BatchNorm-PyTorch
cp -rf Synchronized-BatchNorm-PyTorch/sync_batchnorm .
cd ../../
```

## Dataset Preparation

There are different modes to load images by specifying `--preprocess_mode` along with `--load_size`. `--crop_size`. There are options such as `resize_and_crop`, which resizes the images into square images of side length `load_size` and randomly crops to `crop_size`. `scale_shortside_and_crop` scales the image to have a short side of length `load_size` and crops to `crop_size` x `crop_size` square. To see all modes, please use `python train.py --help` and take a look at `data/base_dataset.py`. By default at the training phase, the images are randomly flipped horizontally. To prevent this use `--no_flip`.

## Generating Images Using Pretrained Model

Once the dataset is ready, the result images can be generated using pretrained models.

1. Requests to access the model weights and fetal datasets should be directed to: [Andras Jakab](andras.jakab@kispi.uzh.ch)
2. Download the pretrained models, save it in 'checkpoints/', and run

    ```
    cd checkpoints
    tar xvf checkpoints.tar.gz
    cd ../
    ```

2. Generate images using the pretrained model.
    ```bash
    python test.py --name [experiment_name] --dataset_mode custom --label_dir [path_to_labels] -- image_dir [path_to_images]
    ```

3. The outputs images are stored at `./results/[type]_pretrained/` by default. You can view them using the autogenerated HTML file in the directory.

## Training New Models

New models can be trained with the following commands.

1. For custom datasets, the easiest way is to use `./data/custom_dataset.py` by specifying the option `--dataset_mode custom`, along with `--label_dir [path_to_labels] --image_dir [path_to_images]`. You also need to specify options such as `--label_nc` for the number of label classes in the dataset, `--contain_dontcare_label` to specify whether it has an unknown label, or `--no_instance` to denote the dataset doesn't have instance maps.

2. Train.

```bash
# To train on your own custom dataset
python train.py --name [experiment_name] --dataset_mode custom --label_dir [path_to_labels] -- image_dir [path_to_images] --label_nc [num_labels]
```

There are many options you can specify. Please use `python train.py --help`. The specified options are printed to the console. To specify the number of GPUs to utilize, use `--gpu_ids`. If you want to use the second and third GPUs for example, use `--gpu_ids 1,2`.

To log training, use `--tf_log` for Tensorboard. The logs are stored at `[checkpoints_dir]/[name]/logs`.

## Testing

Testing is similar to testing pretrained models.

```bash
python test.py --name [name_of_experiment] --dataset_mode [dataset_mode] --dataroot [path_to_dataset]
```

Use `--results_dir` to specify the output directory. `--how_many` will specify the maximum number of images to generate. By default, it loads the latest checkpoint. It can be changed using `--which_epoch`.

## Code Structure

- `train.py`, `test.py`: the entry point for training and testing.
- `trainers/pix2pix_trainer.py`: harnesses and reports the progress of training.
- `models/pix2pix_model.py`: creates the networks, and compute the losses
- `models/networks/`: defines the architecture of all models
- `options/`: creates option lists using `argparse` package. More individuals are dynamically added in other files as well. Please see the section below.
- `data/`: defines the class for loading images and label maps.


